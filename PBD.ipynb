{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from random import shuffle\n",
    "from math import floor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize']=(14,8)\n",
    "\n",
    "global_config = {    \n",
    "    'n_samples': 747,\n",
    "    'n_features': 25,\n",
    "    'n_experiments': 1,\n",
    "    \n",
    "    'train_portion': 0.8,    \n",
    "    'n_hidden_units_per_layer': 200,\n",
    "    'batch_size': 128,\n",
    "    'n_repetitions': 1000, \n",
    "    \n",
    "    'dropout_keep_input': 0.8,\n",
    "    'dropout_keep_hidden': 0.8,\n",
    "    'log_level': 100\n",
    "}\n",
    "\n",
    "global_config['n_train'] = floor(global_config['n_samples'] * global_config['train_portion'])\n",
    "global_config['n_test'] = global_config['n_samples'] - global_config['n_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    p = np.array(p)\n",
    "    return -p * np.log(p) - (1.0-p) * np.log(1.0-p)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def init_weights_xavier(shape):\n",
    "    initializer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "    return tf.Variable(initializer(shape=shape))\n",
    "\n",
    "def compute_PEHE(TE_true, TE_predict):\n",
    "    return np.sqrt(np.mean(np.abs(TE_true-TE_predict)**2))\n",
    "\n",
    "def estimate_propensities(Dataset):\n",
    "    X_train = Dataset.drop(['Treatment','Response','TE'],axis=1)\n",
    "    y_train = Dataset['Treatment']\n",
    "    logmodel     = LogisticRegression()\n",
    "    logmodel.fit(X_train,y_train)\n",
    "    PScores      = logmodel.predict_proba(X_train)\n",
    "    Propensities = np.transpose(PScores)[1,]\n",
    "    Dataset['Propensity'] = Propensities\n",
    "    Dataset['Entropy']    = -Propensities*np.log(Propensities)-(1-Propensities)*np.log(1-Propensities)\n",
    "    return Dataset\n",
    "\n",
    "def parse_result_list(result_list, exp_name):\n",
    "    avg_pehe_train = np.mean([np.min(res['pehe_train_vals']) for res in result_list])\n",
    "    std_pehe_train = np.std([np.min(res['pehe_train_vals']) for res in result_list])\n",
    "    avg_pehe_test = np.mean([np.min(res['pehe_test_vals']) for res in result_list])\n",
    "    std_pehe_test = np.std([np.min(res['pehe_test_vals']) for res in result_list])\n",
    "\n",
    "    print(\"{}:\".format(exp_name))\n",
    "    print(\"\\t Train: Mean={0:.5f}, Std={1:.5f})\\t\\tTest: Mean={2:.5f} Std={3:.5f}\".format(avg_pehe_train, std_pehe_train, avg_pehe_test, std_pehe_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate IHDP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def Draw_IHDP():\n",
    "    Raw_Data = pd.read_csv('ihdp_sample.csv')\n",
    "    X        = np.array(Raw_Data[['X5','X6','X7','X8','X9','X10','X11','X12','X13','X14','X15','X16','X17','X18','X19','X20','X21','X22','X23','X24','X25','X26','X27','X28','X29']])\n",
    "    W        = np.array(Raw_Data['Treatment'])\n",
    "    BetaB    = np.random.choice([0, 0.1, 0.2, 0.3, 0.4],size=25,replace=True,p=[0.6, 0.1, 0.1, 0.1,0.1])\n",
    "    Y_0      = np.random.normal(size=len(X)) + np.exp(np.dot(X+0.5,BetaB))\n",
    "    Y_1      = np.random.normal(size=len(X)) + np.dot(X,BetaB)\n",
    "    AVG      = np.mean(Y_1[W==1]-Y_0[W==1])\n",
    "    Y_1      = Y_1-AVG+4  \n",
    "    TE       = np.dot(X,BetaB)-AVG+4-np.exp(np.dot(X+0.5,BetaB))\n",
    "    Y        = np.transpose(np.array([W,(1-W)*Y_0+W*Y_1,TE]))\n",
    "    DatasetX = pd.DataFrame(X,columns='X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14 X15 X16 X17 X18 X19 X20 X21 X22 X23 X24 X25'.split())\n",
    "    DatasetY = pd.DataFrame(Y,columns='Treatment Response TE'.split())\n",
    "    Dataset  = DatasetX.join(DatasetY)                        \n",
    "    return Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN-4 Dropout (Treatment as Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_4(X, w_h1, w_h2, w_h3, w_h4, w_o, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    \n",
    "    # 1st hidden layer\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w_h1))\n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    \n",
    "    # 2nd hidden layer\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w_h2))\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    \n",
    "    # 3rd hidden layer\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, w_h3))\n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    \n",
    "    # 4th hidden layer\n",
    "    h4 = tf.nn.relu(tf.matmul(h3, w_h4))\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    \n",
    "    # Output\n",
    "    y_out = tf.nn.relu(tf.matmul(h4, w_o))\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_nn4(dataset_train, dataset_test, no_dropout=False, show_log=False):\n",
    "\n",
    "    # Define Placeholders and Init Weights\n",
    "    nn4_X = tf.placeholder(\"float\", [None, n_features + 1])\n",
    "    nn4_Y = tf.placeholder(\"float\", [None, 1])\n",
    "    \n",
    "    nn4_p_keep_input = tf.placeholder(\"float\")\n",
    "    nn4_p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "    nn4_w_h1 = init_weights([n_features + 1, n_hidden_layer_size])\n",
    "    nn4_w_h2 = init_weights([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_w_h3 = init_weights([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_w_h4 = init_weights([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_w_o = init_weights([n_hidden_layer_size, 1])\n",
    "\n",
    "    nn4_pred = NN_4(nn4_X, nn4_w_h1, nn4_w_h2, nn4_w_h3, nn4_w_h4, nn4_w_o, nn4_p_keep_input, nn4_p_keep_hidden)\n",
    "    nn4_costs = tf.reduce_mean(tf.square(nn4_pred - nn4_Y))\n",
    "    nn4_train_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(nn4_costs)\n",
    "    \n",
    "    nn4_X_train = dataset_train.drop(['Response', 'TE', 'Propensity', 'Entropy'], axis=1).as_matrix() # including treatment\n",
    "    nn4_X_0_train = dataset_train.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1)\n",
    "    nn4_X_0_train['Treatment'] = 0.0\n",
    "    nn4_X_1_train = dataset_train.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1)\n",
    "    nn4_X_1_train['Treatment'] = 1.0\n",
    "\n",
    "    nn4_X_test = dataset_test.drop(['Response', 'TE', 'Propensity', 'Entropy'], axis=1).as_matrix() # including treatment\n",
    "    nn4_X_0_test = dataset_test.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1)\n",
    "    nn4_X_0_test['Treatment'] = 0.0\n",
    "    nn4_X_1_test = dataset_test.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1)\n",
    "    nn4_X_1_test['Treatment'] = 1.0\n",
    "\n",
    "    TE_train = np.reshape(dataset_train['TE'].as_matrix(), [n_train, 1])\n",
    "    TE_test = np.reshape(dataset_test['TE'].as_matrix(), [n_test, 1])\n",
    "    \n",
    "    Y_train = np.reshape(dataset_train['Response'].as_matrix(), [n_train, 1])\n",
    "    Y_test = np.reshape(dataset_test['Response'].as_matrix(), [n_test, 1])\n",
    "\n",
    "    nn_dropout_keep_hidden = dropout_keep_hidden\n",
    "    nn_dropout_keep_input = dropout_keep_input\n",
    "\n",
    "    # Parse Dropout Flag\n",
    "    if no_dropout:\n",
    "        nn_dropout_keep_hidden = 1.0\n",
    "        nn_dropout_keep_input = 1.0\n",
    "        \n",
    "    \n",
    "    # Start Training\n",
    "    nn4_mses_train = []\n",
    "    nn4_pehe_train = []\n",
    "    nn4_pehe_test = []\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(n_repetitions):\n",
    "\n",
    "            # Training\n",
    "            for start, end in zip(range(0, len(nn4_X_train), batch_size), range(batch_size, len(nn4_X_train)+1, batch_size)):            \n",
    "                sess.run(nn4_train_op, feed_dict={nn4_X: nn4_X_train[start:end], \n",
    "                                              nn4_Y: Y_train[start:end],\n",
    "                                              nn4_p_keep_input: nn_dropout_keep_input,\n",
    "                                              nn4_p_keep_hidden: nn_dropout_keep_hidden})\n",
    "\n",
    "            # Compute Training Error\n",
    "            mse_train = sess.run(nn4_costs, feed_dict={nn4_X: nn4_X_train, nn4_Y: Y_train, nn4_p_keep_input: 1.0,  nn4_p_keep_hidden: 1.0})\n",
    "            nn4_mses_train.append(mse_train)\n",
    "\n",
    "            # Predict TE \n",
    "            # ... on training\n",
    "            Y0_predict_train = sess.run(nn4_pred, feed_dict={nn4_X: nn4_X_0_train, nn4_Y: Y_train,\n",
    "                                                             nn4_p_keep_input: 1.0,  nn4_p_keep_hidden: 1.0})\n",
    "            Y1_predict_train = sess.run(nn4_pred, feed_dict={nn4_X: nn4_X_1_train, nn4_Y: Y_train,\n",
    "                                                             nn4_p_keep_input: 1.0,  nn4_p_keep_hidden: 1.0})\n",
    "\n",
    "            # ... on test\n",
    "            Y0_predict_test = sess.run(nn4_pred, feed_dict={nn4_X: nn4_X_0_test, nn4_Y: Y_test,\n",
    "                                                            nn4_p_keep_input: 1.0,  nn4_p_keep_hidden: 1.0})\n",
    "            Y1_predict_test = sess.run(nn4_pred, feed_dict={nn4_X: nn4_X_1_test, nn4_Y: Y_test,\n",
    "                                                            nn4_p_keep_input: 1.0,  nn4_p_keep_hidden: 1.0})\n",
    "\n",
    "\n",
    "            TE_true_train              = TE_train\n",
    "            TE_predict_train           = np.array(Y1_predict_train)-np.array(Y0_predict_train)\n",
    "\n",
    "            TE_true_test               = TE_test\n",
    "            TE_predict_test            = np.array(Y1_predict_test)-np.array(Y0_predict_test)\n",
    "\n",
    "            # Compute PEHE\n",
    "            pehe_train = compute_PEHE(TE_true_train, TE_predict_train)\n",
    "            nn4_pehe_train.append(pehe_train)\n",
    "\n",
    "            pehe_test = compute_PEHE(TE_true_test, TE_predict_test)\n",
    "            nn4_pehe_test.append(pehe_test)\n",
    "\n",
    "\n",
    "            if show_log and i % log_level == 0:\n",
    "                print('#{}. \\tMSE (Train): {}\\t PEHE (Train): {}\\t PEHE (Test): {}'.format(i, mse_train, pehe_train, pehe_test))\n",
    "    \n",
    "    result_dict = {\n",
    "        \"pehe_train_vals\": nn4_pehe_train,\n",
    "        \"pehe_test_vals\": nn4_pehe_test\n",
    "    }\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN-4 Dropout (with 2 outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def NN_4_multi_4s(X, weights, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    \n",
    "    # 1st hidden layer (Shared)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, weights['w_h1']))\n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    \n",
    "    # 2nd hidden layer (Shared)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, weights['w_h2']))\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    \n",
    "    # 3rd hidden layer (Shared)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, weights['w_h3']))\n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    \n",
    "    # 4th hidden layer (Shared)\n",
    "    h4 = tf.nn.relu(tf.matmul(h3, weights['w_h4']))\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    \n",
    "    # Output\n",
    "    Y_0_out = tf.matmul(h4, weights['w_out_0']) \n",
    "    Y_1_out = tf.matmul(h4, weights['w_out_1'])\n",
    "    \n",
    "    return Y_0_out, Y_1_out\n",
    "\n",
    "def NN_4_multi_2s_2i(X, weights, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    \n",
    "    # 1st hidden layer (Shared)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, weights['w_h1']))\n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    \n",
    "    # 2nd hidden layer (Shared)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, weights['w_h2']))\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    \n",
    "    # 3rd hidden layer (idiosyncratic for Y0)\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, weights['w_h3']))\n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    \n",
    "    # 4th hidden layer (idiosyncratic for Y1)\n",
    "    h4 = tf.nn.relu(tf.matmul(h2, weights['w_h4']))\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    \n",
    "    # Output\n",
    "    Y_0_out = tf.matmul(h3, weights['w_out_0']) \n",
    "    Y_1_out = tf.matmul(h4, weights['w_out_1'])\n",
    "    \n",
    "    return Y_0_out, Y_1_out\n",
    "\n",
    "def NN_4_multi_2s_4i(X, weights, p_keep_input, p_keep_hidden):\n",
    "    X = tf.nn.dropout(X, p_keep_input)\n",
    "    \n",
    "    # 1st hidden layer (Shared)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, weights['w_h1']))\n",
    "    h1 = tf.nn.dropout(h1, p_keep_hidden)\n",
    "    \n",
    "    # 2nd hidden layer (Shared)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, weights['w_h2']))\n",
    "    h2 = tf.nn.dropout(h2, p_keep_hidden)\n",
    "    \n",
    "    # 3rd hidden layer  (idiosyncratic for Y0) -- 1\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, weights['w_h3']))\n",
    "    h3 = tf.nn.dropout(h3, p_keep_hidden)\n",
    "    \n",
    "    # 3rd hidden layer (idiosyncratic for Y0)  -- 2\n",
    "    h3_2 = tf.nn.relu(tf.matmul(h3, weights['w_h3_2']))\n",
    "    h3_2 = tf.nn.dropout(h3_2, p_keep_hidden)\n",
    "    \n",
    "    # 4th hidden layer (idiosyncratic for Y1) -- 1\n",
    "    h4 = tf.nn.relu(tf.matmul(h2, weights['w_h4']))\n",
    "    h4 = tf.nn.dropout(h4, p_keep_hidden)\n",
    "    \n",
    "    # 4th hidden layer (idiosyncratic for Y1) -- 2\n",
    "    h4_2 = tf.nn.relu(tf.matmul(h4, weights['w_h4_2']))\n",
    "    h4_2 = tf.nn.dropout(h4_2, p_keep_hidden)\n",
    "    \n",
    "    \n",
    "    # Output\n",
    "    Y_0_out = tf.matmul(h3_2, weights['w_out_0']) \n",
    "    Y_1_out = tf.matmul(h4_2, weights['w_out_1'])\n",
    "    \n",
    "    return Y_0_out, Y_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_nn4_multi(dataset_train, dataset_test, no_dropout=False, show_log=False, architecture='4s'):\n",
    "    \n",
    "    # Define Placeholders and Init Weights\n",
    "    X = tf.placeholder(\"float\", [None, global_config['n_features']])\n",
    "    Y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "    p_keep_input = tf.placeholder(\"float\")\n",
    "    p_keep_hidden = tf.placeholder(\"float\")\n",
    "\n",
    "    Y_0         = tf.placeholder(\"float\", shape=[None, 1])       # Task 1 output\n",
    "    Y_1         = tf.placeholder(\"float\", shape=[None, 1])       # Task 2 output\n",
    "\n",
    "    weights = {\n",
    "        'w_h1': init_weights_xavier([global_config['n_features'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_h2': init_weights_xavier([global_config['n_hidden_units_per_layer'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_h3': init_weights_xavier([global_config['n_hidden_units_per_layer'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_h3_2': init_weights_xavier([global_config['n_hidden_units_per_layer'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_h4': init_weights_xavier([global_config['n_hidden_units_per_layer'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_h4_2': init_weights_xavier([global_config['n_hidden_units_per_layer'], global_config['n_hidden_units_per_layer']]),\n",
    "        'w_out_0': init_weights_xavier([global_config['n_hidden_units_per_layer'], 1]),\n",
    "        'w_out_1': init_weights_xavier([global_config['n_hidden_units_per_layer'], 1])\n",
    "    }\n",
    "\n",
    "    # Parse Shared Layers Flag\n",
    "    if architecture == '4s':\n",
    "        pred_Y0, pred_Y1 = NN_4_multi_4s(X, weights, p_keep_input, p_keep_hidden)\n",
    "    elif architecture == '4s_2i':\n",
    "        pred_Y0, pred_Y1 = NN_4_multi_2s_2i(X, weights, p_keep_input, p_keep_hidden) \n",
    "    elif architecture == '4s_4i':\n",
    "        pred_Y0, pred_Y1 = NN_4_multi_2s_4i(X, weights, p_keep_input, p_keep_hidden) \n",
    "    else: \n",
    "        raise Exception('Invalid architecture passed ({})'.format(architecture))\n",
    "\n",
    "    cost0     = tf.nn.l2_loss(Y_0-pred_Y0)\n",
    "    cost1     = tf.nn.l2_loss(Y_1-pred_Y1)\n",
    "\n",
    "    optim0    = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost0)\n",
    "    optim1    = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost1)\n",
    "    \n",
    "    # Prepare Datasets\n",
    "    X_train = dataset_train.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1).as_matrix() \n",
    "    TE_train = np.reshape(dataset_train['TE'].as_matrix(), [global_config['n_train'], 1])\n",
    "\n",
    "    X_test = dataset_test.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1).as_matrix() \n",
    "    TE_test = np.reshape(dataset_test['TE'].as_matrix(), [global_config['n_test'], 1])\n",
    "\n",
    "\n",
    "    \n",
    "    Y_train = np.reshape(dataset_train['Response'].as_matrix(), [global_config['n_train'], 1])\n",
    "    Y_test = np.reshape(dataset_test['Response'].as_matrix(), [global_config['n_test'], 1])\n",
    "    \n",
    "    # Parse Dropout Flag\n",
    "    if no_dropout:\n",
    "        dropout_keep_input = 1.0\n",
    "        dropout_keep_hidden = 1.0\n",
    "    else:\n",
    "        dropout_keep_input = global_config['dropout_keep_input']\n",
    "        dropout_keep_hidden = global_config['dropout_keep_hidden']\n",
    "        \n",
    "    \n",
    "    # Start Training\n",
    "    mses_train_0 = []\n",
    "    mses_train_1 = []\n",
    "\n",
    "    pehes_train = []\n",
    "    pehes_test = []\n",
    "\n",
    "    Y0_loss = 0\n",
    "    Y1_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(global_config['n_repetitions']):\n",
    "            if i%2 == 0:\n",
    "                _, Y0_loss = sess.run([optim0, cost0], feed_dict= {\n",
    "                              X  : X_train[dataset_train['Treatment']==0], \n",
    "                              Y_0: Y_train[dataset_train['Treatment']==0],\n",
    "                              p_keep_input: dropout_keep_input,\n",
    "                              p_keep_hidden: dropout_keep_hidden})\n",
    "            else:\n",
    "                _, Y1_loss = sess.run([optim1, cost1], feed_dict= {\n",
    "                              X  : X_train[dataset_train['Treatment']==1], \n",
    "                              Y_1: Y_train[dataset_train['Treatment']==1],\n",
    "                              p_keep_input: dropout_keep_input,\n",
    "                              p_keep_hidden: dropout_keep_hidden})\n",
    "\n",
    "            # Compute Training Error\n",
    "            mses_train_0.append(Y0_loss)\n",
    "            mses_train_1.append(Y1_loss)\n",
    "\n",
    "\n",
    "            # Predict TE\n",
    "            Y0_predict_train = sess.run(pred_Y0, feed_dict={X: X_train, p_keep_input: 1.0,\n",
    "                              p_keep_hidden: 1.0})\n",
    "            Y1_predict_train = sess.run(pred_Y1, feed_dict={X: X_train, p_keep_input: 1.0,\n",
    "                              p_keep_hidden: 1.0})\n",
    "\n",
    "            TE_true_train               = TE_train\n",
    "            TE_predict_train            = np.array(Y1_predict_train)-np.array(Y0_predict_train)\n",
    "\n",
    "            Y0_predict_test = sess.run(pred_Y0, feed_dict={X: X_test, p_keep_input: 1.0,\n",
    "                              p_keep_hidden: 1.0})\n",
    "            Y1_predict_test = sess.run(pred_Y1, feed_dict={X: X_test, p_keep_input: 1.0,\n",
    "                              p_keep_hidden: 1.0})\n",
    "\n",
    "            TE_true_test               = TE_test\n",
    "            TE_predict_test            = np.array(Y1_predict_test)-np.array(Y0_predict_test)\n",
    "\n",
    "            # Compute PEHE\n",
    "            pehe_train = compute_PEHE(TE_true_train, TE_predict_train)\n",
    "            pehes_train.append(pehe_train)\n",
    "\n",
    "            pehe_test = compute_PEHE(TE_true_test, TE_predict_test)\n",
    "            pehes_test.append(pehe_test)\n",
    "\n",
    "            if show_log and i % log_level == 0:\n",
    "                print('#{}. \\tMSE Y0: {} \\tMSE Y1: {}\\t PEHE: {}'.format(i, Y0_loss, Y1_loss, pehe_train))\n",
    "\n",
    "        sess.close()\n",
    "\n",
    "    result_dict = {\n",
    "        \"pehe_train_vals\": pehes_train,\n",
    "        \"pehe_test_vals\": pehe_test\n",
    "    }\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN-4 Propensity-Based Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_4_PBD(X, entropy, rand_mask_X, rand_mask_h1, rand_mask_h2, rand_mask_h3, rand_mask_h4, w_h1, w_h2, w_h3, w_h4, w_out_0, w_out_1):\n",
    "    \n",
    "    # Compute Keep Probability based on entropy\n",
    "    keep_prop = entropy + 0.5\n",
    "\n",
    "    # Input Layer\n",
    "    pbd_mask_X = tf.to_float(tf.greater_equal(keep_prop, rand_mask_X))\n",
    "    X = tf.multiply(X, pbd_mask_X)\n",
    "    #w_h1_rescaled = w_h1 * (1.0/keep_prop)\n",
    "    \n",
    "    # 1st hidden layer (Shared)\n",
    "    h1 = tf.nn.relu(tf.matmul(X, w_h1))\n",
    "    pbd_mask_h1 = tf.to_float(tf.greater_equal(keep_prop, rand_mask_h1))\n",
    "    #h1 = tf.multiply(h1, pbd_mask_h1)\n",
    "    #w_h2_rescaled = w_h2 * (1.0/keep_prop)\n",
    "    \n",
    "    # 2nd hidden layer (Shared)\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w_h2))\n",
    "    pbd_mask_h2 = tf.to_float(tf.greater_equal(keep_prop, rand_mask_h2))\n",
    "    #h2 = tf.multiply(h2, pbd_mask_h2)\n",
    "    #w_h3_rescaled = w_h3 * (1.0/keep_prop)\n",
    "\n",
    "    # 3rd hidden layer\n",
    "    h3 = tf.nn.relu(tf.matmul(h2, w_h3))\n",
    "    pbd_mask_h3 = tf.to_float(tf.greater_equal(keep_prop, rand_mask_h3))\n",
    "    #h3 = tf.multiply(h3, pbd_mask_h3)\n",
    "    #w_h4_rescaled = w_h4 * (1.0/keep_prop)\n",
    "\n",
    "    # 4th hidden layer\n",
    "    h4 = tf.nn.relu(tf.matmul(h3, w_h4))\n",
    "    pbd_mask_h4 = tf.to_float(tf.greater_equal(keep_prop, rand_mask_h4))\n",
    "    #h4 = tf.multiply(h4, pbd_mask_h3)\n",
    "    #w_out_0_rescaled = w_out_0 * (1.0/keep_prop)\n",
    "    #w_out_1_rescaled = w_out_1 * (1.0/keep_prop)\n",
    "    \n",
    "    # Output\n",
    "    Y_0_out = tf.matmul(h4, w_out_0) \n",
    "    Y_1_out = tf.matmul(h4, w_out_1)\n",
    "    \n",
    "    return Y_0_out, Y_1_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_nn4_pbd(dataset_train, dataset_test, show_log=False):\n",
    "    \n",
    "    # Define Placeholders and Init Weights\n",
    "    nn4_pbd_X = tf.placeholder(\"float\", [None, n_features])\n",
    "    nn4_pbd_Entropy = tf.placeholder(\"float\", [None, 1])\n",
    "    #nn4_pbd_rand_mask = tf.placeholder(\"float\", [None, n_features])\n",
    "    \n",
    "    nn4_pbd_rand_mask_X = tf.placeholder(\"float\", [None, n_features])\n",
    "    nn4_pbd_rand_mask_h1 = tf.placeholder(\"float\", [None, n_hidden_layer_size])\n",
    "    nn4_pbd_rand_mask_h2 = tf.placeholder(\"float\", [None, n_hidden_layer_size])\n",
    "    nn4_pbd_rand_mask_h3 = tf.placeholder(\"float\", [None, n_hidden_layer_size])\n",
    "    nn4_pbd_rand_mask_h4 = tf.placeholder(\"float\", [None, n_hidden_layer_size])\n",
    "    \n",
    "\n",
    "    nn4_pbd_Y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "    nn4_pbd_Y_0         = tf.placeholder(\"float\", shape=[None, 1])       # Task 1 output\n",
    "    nn4_pbd_Y_1         = tf.placeholder(\"float\", shape=[None, 1])       # Task 2 output\n",
    "\n",
    "    nn4_pbd_w_h1 = init_weights_xavier([n_features, n_hidden_layer_size])\n",
    "    nn4_pbd_w_h2 = init_weights_xavier([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_pbd_w_h3 = init_weights_xavier([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_pbd_w_h4 = init_weights_xavier([n_hidden_layer_size, n_hidden_layer_size])\n",
    "    nn4_pbd_w_out_0 = init_weights_xavier([n_hidden_layer_size, 1])\n",
    "    nn4_pbd_w_out_1 = init_weights_xavier([n_hidden_layer_size, 1])\n",
    "\n",
    "\n",
    "    nn4_pbd_pred_Y0, nn4_pbd_pred_Y1 = NN_4_PBD(nn4_pbd_X, nn4_pbd_Entropy, nn4_pbd_rand_mask_X, nn4_pbd_rand_mask_h1, nn4_pbd_rand_mask_h2,\n",
    "                                       nn4_pbd_rand_mask_h3, nn4_pbd_rand_mask_h4, nn4_pbd_w_h1, nn4_pbd_w_h2, nn4_pbd_w_h3, nn4_pbd_w_h4, \n",
    "                                       nn4_pbd_w_out_0, nn4_pbd_w_out_1)\n",
    "\n",
    "\n",
    "    \n",
    "    nn4_pbd_cost0     = tf.nn.l2_loss(nn4_pbd_Y_0-nn4_pbd_pred_Y0)\n",
    "    nn4_pbd_cost1     = tf.nn.l2_loss(nn4_pbd_Y_1-nn4_pbd_pred_Y1)\n",
    "\n",
    "    nn4_pbd_optim0    = tf.train.AdamOptimizer(learning_rate=0.001).minimize(nn4_pbd_cost0)\n",
    "    nn4_pbd_optim1    = tf.train.AdamOptimizer(learning_rate=0.001).minimize(nn4_pbd_cost1)\n",
    "\n",
    "\n",
    "    nn4_pbd_X_train = dataset_train.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1).as_matrix() \n",
    "    TE_train = np.reshape(dataset_train['TE'].as_matrix(), [n_train, 1])\n",
    "    nn4_pbd_Entropy_train =  np.reshape(dataset_train['Entropy'].as_matrix(), [n_train, 1])\n",
    "\n",
    "    nn4_pbd_X_test = dataset_test.drop(['Response', 'TE', 'Treatment', 'Propensity', 'Entropy'], axis=1).as_matrix() \n",
    "    TE_test = np.reshape(dataset_test['TE'].as_matrix(), [n_test, 1])\n",
    "    nn4_pbd_Entropy_test =  np.reshape(dataset_test['Entropy'].as_matrix(), [n_test, 1])\n",
    "    \n",
    "    Y_train = np.reshape(dataset_train['Response'].as_matrix(), [n_train, 1])\n",
    "    Y_test = np.reshape(dataset_test['Response'].as_matrix(), [n_test, 1])\n",
    "\n",
    "\n",
    "    # Start Training\n",
    "    nn4_pbd_mses_train_0 = []\n",
    "    nn4_pbd_mses_train_1 = []\n",
    "\n",
    "    nn4_pbd_pehe_train = []\n",
    "    nn4_pbd_pehe_test = []\n",
    "\n",
    "\n",
    "    Y0_loss = 0\n",
    "    Y1_loss = 0\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for i in range(n_repetitions):\n",
    "\n",
    "            if i%2 == 0:\n",
    "                n = Y_train[dataset_train['Treatment']==0].shape[0]\n",
    "                _, Y0_loss = sess.run([nn4_pbd_optim0, nn4_pbd_cost0], feed_dict= {\n",
    "                              nn4_pbd_X  : nn4_pbd_X_train[dataset_train['Treatment']==0], \n",
    "                              nn4_pbd_Y_0: Y_train[dataset_train['Treatment']==0], \n",
    "                              nn4_pbd_Entropy: nn4_pbd_Entropy_train[dataset_train['Treatment']==0],\n",
    "                              nn4_pbd_rand_mask_X: np.random.uniform(size=[n, n_features]),\n",
    "                              nn4_pbd_rand_mask_h1: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h2: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h3: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h4: np.random.uniform(size=[n, n_hidden_layer_size])})\n",
    "            else:\n",
    "                n = Y_train[dataset_train['Treatment']==1].shape[0]\n",
    "                _, Y1_loss = sess.run([nn4_pbd_optim1, nn4_pbd_cost1], feed_dict= {\n",
    "                              nn4_pbd_X  : nn4_pbd_X_train[dataset_train['Treatment']==1], \n",
    "                              nn4_pbd_Y_1: Y_train[dataset_train['Treatment']==1],\n",
    "                              nn4_pbd_Entropy: nn4_pbd_Entropy_train[dataset_train['Treatment']==1],\n",
    "                              nn4_pbd_rand_mask_X: np.random.uniform(size=[n, n_features]),\n",
    "                              nn4_pbd_rand_mask_h1: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h2: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h3: np.random.uniform(size=[n, n_hidden_layer_size]),\n",
    "                              nn4_pbd_rand_mask_h4: np.random.uniform(size=[n, n_hidden_layer_size])})\n",
    "\n",
    "            # Compute Training Error\n",
    "            nn4_pbd_mses_train_0.append(Y0_loss)\n",
    "            nn4_pbd_mses_train_1.append(Y1_loss)\n",
    "\n",
    "            # Predict TE\n",
    "            # ... on Training Set\n",
    "            Y0_predict_train = sess.run(nn4_pbd_pred_Y0, feed_dict={nn4_pbd_X: nn4_pbd_X_train, \n",
    "                                                              nn4_pbd_rand_mask_X: np.ones((n_train, n_features)),\n",
    "                                                              nn4_pbd_rand_mask_h1: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h2: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h3: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h4: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_Entropy: 0.5 * np.ones((n_train, 1))\n",
    "                                                             })\n",
    "            Y1_predict_train = sess.run(nn4_pbd_pred_Y1, feed_dict={nn4_pbd_X: nn4_pbd_X_train, \n",
    "                                                              nn4_pbd_rand_mask_X: np.ones((n_train, n_features)),\n",
    "                                                              nn4_pbd_rand_mask_h1: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h2: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h3: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h4: np.ones((n_train, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_Entropy: 0.5 * np.ones((n_train, 1))})\n",
    "\n",
    "            TE_true_train               = TE_train\n",
    "            TE_predict_train            = np.array(Y1_predict_train)-np.array(Y0_predict_train)\n",
    "\n",
    "            # ... on Test Set\n",
    "            Y0_predict_test = sess.run(nn4_pbd_pred_Y0, feed_dict={nn4_pbd_X: nn4_pbd_X_test, \n",
    "                                                              nn4_pbd_rand_mask_X: np.ones((n_test, n_features)),\n",
    "                                                              nn4_pbd_rand_mask_h1: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h2: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h3: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h4: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_Entropy: 0.5 * np.ones((n_test, 1))\n",
    "                                                             })\n",
    "            Y1_predict_test = sess.run(nn4_pbd_pred_Y1, feed_dict={nn4_pbd_X: nn4_pbd_X_test, \n",
    "                                                              nn4_pbd_rand_mask_X: np.ones((n_test, n_features)),\n",
    "                                                              nn4_pbd_rand_mask_h1: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h2: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h3: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_rand_mask_h4: np.ones((n_test, n_hidden_layer_size)),\n",
    "                                                              nn4_pbd_Entropy: 0.5 * np.ones((n_test, 1))})\n",
    "\n",
    "            TE_true_test               = TE_test\n",
    "            TE_predict_test            = np.array(Y1_predict_test)-np.array(Y0_predict_test)\n",
    "\n",
    "            # Compute PEHE\n",
    "            pehe_train = compute_PEHE(TE_true_train, TE_predict_train)\n",
    "            nn4_pbd_pehe_train.append(pehe_train)\n",
    "\n",
    "            pehe_test = compute_PEHE(TE_true_test, TE_predict_test)\n",
    "            nn4_pbd_pehe_test.append(pehe_test)\n",
    "\n",
    "\n",
    "            if show_log and i % log_level == 0:\n",
    "                print('#{}. \\tMSE Y0: {} \\tMSE Y1: {}\\t PEHE (Train): {}\\t PEHE (Test): {}'.format(i, floor(Y0_loss), floor(Y1_loss), pehe_train, pehe_test))\n",
    "\n",
    "        sess.close()\n",
    "    \n",
    "    result_dict = {\n",
    "        \"pehe_train_vals\": nn4_pbd_pehe_train,\n",
    "        \"pehe_test_vals\": nn4_pbd_pehe_test\n",
    "    }\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiment 1/1.\n",
      "\tMin. PEHE (Train). NN4 Multi 4s: 1.6342765060023081\t NN4 Multi 2s 2i: 1.5229579602154808\t NN4 Multi 2s 4i: 1.5212343479190946\n",
      "\tMin. PEHE (Test). NN4 Multi 4s: 2.1181105144753998\t NN4 Multi 2s 2i: 1.7192372989016738\t NN4 Multi 2s 4i: 1.5728535758792503\n"
     ]
    }
   ],
   "source": [
    "nn4_all_results = []               # NN4 (Dropout)\n",
    "nn4_no_dropout_all_results = []    # NN4 (no Dropout)\n",
    "nn4_multi_4s_all_results = []      # NN4 2 outcomes 4 shared\n",
    "nn4_multi_2s_2i_all_results = []   # NN4 2 outcomes 2 shared, 2 idiosyncratic\n",
    "nn4_multi_2s_4i_all_results = []   # NN4 2 outcomes 2 shared, 2 idiosyncratic\n",
    "nn4_pbd_all_results = []           # NN4 2 outcomes (PBD)\n",
    "\n",
    "for i in range(global_config['n_experiments']):\n",
    "    print('Running Experiment {}/{}.'.format(i+1, global_config['n_experiments']))\n",
    "    \n",
    "    # Draw Data\n",
    "    dataset = Draw_IHDP()\n",
    "    estimate_propensities(dataset)\n",
    "    \n",
    "    # Shuffle Dataset\n",
    "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Split Data into test and training set\n",
    "    dataset_train = dataset[0:global_config['n_train']]\n",
    "    dataset_test = dataset[global_config['n_train']:]\n",
    "    \n",
    "    # NN4 (Dropout)\n",
    "    #nn4_results = run_nn4(dataset_train, dataset_test)\n",
    "    #nn4_all_results.append(nn4_results)\n",
    "    #nn4_min_pehe_train =  np.min(nn4_results['pehe_train_vals'])\n",
    "    #nn4_min_pehe_test =  np.min(nn4_results['pehe_test_vals'])\n",
    "\n",
    "    # NN4 (no Dropout)\n",
    "    #nn4_no_dropout_results = run_nn4(dataset_train, dataset_test, no_dropout=True)\n",
    "    #nn4_no_dropout_all_results.append(nn4_no_dropout_results)\n",
    "    #nn4_no_dropout_min_pehe_train =  np.min(nn4_no_dropout_results['pehe_train_vals'])\n",
    "    #nn4_no_dropout_min_pehe_test =  np.min(nn4_no_dropout_results['pehe_test_vals'])\n",
    "    \n",
    "    # NN4 2 outcomes 4 shared layers(Dropout)\n",
    "    nn4_multi_4s_results = run_nn4_multi(dataset_train, dataset_test, architecture='4s')\n",
    "    nn4_multi_4s_all_results.append(nn4_multi_4s_results)    \n",
    "    nn4_multi_4s_min_pehe_train =  np.min(nn4_multi_4s_results['pehe_train_vals'])\n",
    "    nn4_multi_4s_min_pehe_test =  np.min(nn4_multi_4s_results['pehe_test_vals'])\n",
    "    \n",
    "    # NN4 2 outcomes 2 shared layers, 1 idiosyncratic per outcome (Dropout)\n",
    "    nn4_multi_2s_2i_results = run_nn4_multi(dataset_train, dataset_test, architecture='4s_2i')\n",
    "    nn4_multi_2s_2i_all_results.append(nn4_multi_2s_2i_results)    \n",
    "    nn4_multi_2s_2i_min_pehe_train =  np.min(nn4_multi_2s_2i_results['pehe_train_vals'])\n",
    "    nn4_multi_2s_2i_min_pehe_test =  np.min(nn4_multi_2s_2i_results['pehe_test_vals'])\n",
    "    \n",
    "    # NN4 2 outcomes 2 shared layers, 2 idiosyncratic per outcome (Dropout)\n",
    "    nn4_multi_2s_4i_results = run_nn4_multi(dataset_train, dataset_test, architecture='4s_4i')\n",
    "    nn4_multi_2s_4i_all_results.append(nn4_multi_2s_4i_results)    \n",
    "    nn4_multi_2s_4i_min_pehe_train =  np.min(nn4_multi_2s_4i_results['pehe_train_vals'])\n",
    "    nn4_multi_2s_4i_min_pehe_test =  np.min(nn4_multi_2s_4i_results['pehe_test_vals'])\n",
    "    \n",
    "    # NN4 2 outcomes (PBD)\n",
    "    #nn4_pbd_results = run_nn4_pbd(dataset_train, dataset_test)\n",
    "    #nn4_pbd_all_results.append(nn4_pbd_results)    \n",
    "    #nn4_pbd_min_pehe_train =  np.min(nn4_pbd_results['pehe_train_vals'])\n",
    "    #nn4_pbd_min_pehe_test =  np.min(nn4_pbd_results['pehe_test_vals'])\n",
    "    \n",
    "\n",
    "    #print('\\tMin. PEHE (Train). NN4: {}\\t NN4 (no Dropout): {}\\t NN4 Dropout: {}\\t NN4 PBD: {}'.format(nn4_min_pehe_train, nn4_no_dropout_min_pehe_train, nn4_dropout_min_pehe_train, nn4_pbd_min_pehe_train))\n",
    "    #print('\\tMin. PEHE (Test). NN4: {}\\t NN4 (no Dropout): {}\\t NN4 Dropout: {}\\t NN4 PBD: {}'.format(nn4_min_pehe_test, nn4_no_dropout_min_pehe_test, nn4_dropout_min_pehe_test, nn4_pbd_min_pehe_test))\n",
    "    \n",
    "    # Only 2 Outcomes\n",
    "    print('\\tMin. PEHE (Train). NN4 Multi 4s: {}\\t NN4 Multi 2s 2i: {}\\t NN4 Multi 2s 4i: {}'.format(nn4_multi_4s_min_pehe_train, nn4_multi_2s_2i_min_pehe_train,nn4_multi_2s_4i_min_pehe_train ))\n",
    "    print('\\tMin. PEHE (Test). NN4 Multi 4s: {}\\t NN4 Multi 2s 2i: {}\\t NN4 Multi 2s 4i: {}'.format(nn4_multi_4s_min_pehe_test, nn4_multi_2s_2i_min_pehe_test,nn4_multi_2s_4i_min_pehe_test ))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Result Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle results into file (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine results\n",
    "results_combined = {\n",
    "    'NN4' : nn4_all_results,\n",
    "    'NN4 No Dropout': nn4_no_dropout_all_results,\n",
    "    'NN4 2 Outcomes': nn4_dropout_all_results,\n",
    "    'NN4 PBD': nn4_pbd_all_results\n",
    "}\n",
    "\n",
    "filename = '16-06-17-results.dat'\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(results_combined, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEHE TRAIN\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn4_dropout_all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c259b5bec8e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_no_dropout_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_dropout_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_pbd_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn4_dropout_all_results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"PEHE TRAIN\")\n",
    "print(['%.2f' % np.min(res['pehe_train_vals']) for res in nn4_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_train_vals']) for res in nn4_no_dropout_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_train_vals']) for res in nn4_dropout_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_train_vals']) for res in nn4_pbd_all_results])\n",
    "\n",
    "print(\"\\nPEHE TEST\")\n",
    "print(['%.2f' % np.min(res['pehe_test_vals']) for res in nn4_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_test_vals']) for res in nn4_no_dropout_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_test_vals']) for res in nn4_dropout_all_results])\n",
    "print(['%.2f' % np.min(res['pehe_test_vals']) for res in nn4_pbd_all_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn4_dropout_all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5c1dbdd93f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmin_pehes_nn4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmin_pehes_nn4_no_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_no_dropout_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmin_pehes_nn4_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_dropout_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmin_pehes_nn4_pbd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnn4_pbd_all_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn4_dropout_all_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "min_pehes_nn4 = [np.min(res['pehe_train_vals']) for res in nn4_all_results]\n",
    "min_pehes_nn4_no_dropout = [np.min(res['pehe_train_vals']) for res in nn4_no_dropout_all_results]\n",
    "min_pehes_nn4_dropout = [np.min(res['pehe_train_vals']) for res in nn4_dropout_all_results]\n",
    "min_pehes_nn4_pbd = [np.min(res['pehe_train_vals']) for res in nn4_pbd_all_results]\n",
    "\n",
    "data = [min_pehes_nn4, min_pehes_nn4_no_dropout, min_pehes_nn4_dropout, min_pehes_nn4_pbd]\n",
    "labels = ['NN4 (with Dropout)', 'NN4 (no Dropout)', 'NN4 2Out (with Dropout)', 'NN4 2Out (PBD)']\n",
    "\n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.title('PEHE Values (Train)')\n",
    "plt.ylabel('PEHE')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test\n",
    "min_pehes_nn4 = [np.min(res['pehe_test_vals']) for res in nn4_all_results]\n",
    "min_pehes_nn4_no_dropout = [np.min(res['pehe_test_vals']) for res in nn4_no_dropout_all_results]\n",
    "min_pehes_nn4_dropout = [np.min(res['pehe_test_vals']) for res in nn4_dropout_all_results]\n",
    "min_pehes_nn4_pbd = [np.min(res['pehe_test_vals']) for res in nn4_pbd_all_results]\n",
    "\n",
    "data = [min_pehes_nn4, min_pehes_nn4_no_dropout, min_pehes_nn4_dropout, min_pehes_nn4_pbd]\n",
    "labels = ['NN4 (with Dropout)', 'NN4 (no Dropout)', 'NN4 2Out (with Dropout)', 'NN4 2Out (PBD)']\n",
    "\n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.title('PEHE Values (TEST)')\n",
    "plt.ylabel('PEHE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEHE Values (over epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fdaae328c269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PEHE: NN-4 (Treatment as Feature) vs. NN-4 Dropout (2 outcomes) vs. NN-4 PBD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn4_all_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NN-4 (Train)'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn4_all_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_test_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NN-4 (Test)'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn4_no_dropout_all_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pehe_train_vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NN-4 (Train)'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plt.title('PEHE: NN-4 (Treatment as Feature) vs. NN-4 Dropout (2 outcomes) vs. NN-4 PBD')\n",
    "plt.plot(nn4_all_results[0]['pehe_train_vals'], label='NN-4 (Train)',  color='r')\n",
    "plt.plot(nn4_all_results[0]['pehe_test_vals'], label='NN-4 (Test)',  color='r', linestyle=':')\n",
    "\n",
    "plt.plot(nn4_no_dropout_all_results[0]['pehe_train_vals'], label='NN-4 (Train)',  color='b')\n",
    "plt.plot(nn4_no_dropout_all_results[0]['pehe_test_vals'], label='NN-4 (Test)',  color='b', linestyle=':')\n",
    "#plt.plot(nn4_all_pehe_train[4], label='NN-4 (Train) 2',  color='b')\n",
    "#plt.plot(nn4_all_pehe_test[4], label='NN-4 (Test) 2',  color='b', linestyle=':')\n",
    "\n",
    "#plt.plot(nn4_dropout_pehe_train, label='NN-4 Dropout (Train)',  color='b')\n",
    "#plt.plot(nn4_dropout_pehe_test, label='NN-4 Dropout (Test)',  color='b', linestyle=':')\n",
    "#plt.plot(nn4_pbd_pehe_train, label='NN-4 PBD (Train)',  color='g')\n",
    "#plt.plot(nn4_pbd_pehe_test, label='NN-4 PBD (Test)',  color='g', linestyle=':')\n",
    "\n",
    "plt.ylabel('PEHE')\n",
    "plt.xlabel('# Iteration')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.title('NN-4 (Treatment as Feature)')\n",
    "#plt.plot(nn4_mses_train, label='MSE (Train)')\n",
    "#plt.ylabel('MSE')\n",
    "#plt.xlabel('# Iteration')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.show()\n",
    "\n",
    "#plt.title('NN-4 Dropout (2 outcomes)')\n",
    "#plt.plot(nn4_dropout_mses_train_0, label='MSE Y0')\n",
    "#plt.plot(nn4_dropout_mses_train_1, label='MSE Y1')\n",
    "#plt.ylabel('MSE')\n",
    "#plt.xlabel('# Iteration')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.show()\n",
    "\n",
    "#plt.title('NN-4 PBD')\n",
    "#plt.plot(nn4_pbd_mses_train_0, label='MSE Y0')\n",
    "#plt.plot(nn4_pbd_mses_train_1, label='MSE Y1')\n",
    "#plt.ylabel('MSE')\n",
    "#plt.xlabel('# Iteration')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ox-dl-py3]",
   "language": "python",
   "name": "conda-env-ox-dl-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
